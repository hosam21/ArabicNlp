# -*- coding: utf-8 -*-
"""NLP_practicala8eaa6982c

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/hosammohammed/nlp-practicala8eaa6982c.889c174d-9e73-4349-b8dd-ae84b8fdca5b.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250510/auto/storage/goog4_request%26X-Goog-Date%3D20250510T023655Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Daba263b47240a2eca059816fa210b73c449b8f5373149814ffc8770344a34a4560d37cc938e505efaabc11e1e09e1b721a3a5eeec3b1d6e79de35abd12aecb98588a7df6baa7898c8457089cc0a26be471d63c9fd13cbae3f10c8fad9f453bcb2583536c24ffd4d2cdfd92cfda20218d5406b8737b3adedfa052a69972115979c934fa87c570f68c1871e0601f74b4d3154942b66a7d5ef71214fc008036101bbd5484d6750f38d228042c66783fe7f211a6c0afdc0f4f393ec5dde03b0cfdcc404de28d680822ef8e7658649c2eb2c4f3677fcb573d304d8e8bc463c97b084c9fe2b689710905702e9c73718b838f9014266cc8765885af6b2a5afb572bdb73
"""

!pip install emoji
!pip install arabic-reshaper
!pip install python-bidi
!pip install imbalanced-learn

import pandas as pd
import numpy as np
import re
import emoji
import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import arabic_reshaper
from bidi.algorithm import get_display
import warnings
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from collections import Counter, OrderedDict
from itertools import groupby
from typing import Literal
warnings.filterwarnings('ignore')
import logging
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import AutoTokenizer, TFAutoModel
import joblib
from imblearn.over_sampling import SMOTE
import scipy.sparse as sparse
from sklearn.metrics import precision_score, recall_score, f1_score

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EmojiHandler:
    def __init__(self):
        # Define emoji sentiment mappings
        self.positive_emojis = {
            # Happy faces
            'ğŸ˜Š': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜„': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜ƒ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ˜€': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜†': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ˜…': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ¤£': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜‚': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ™‚': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜‰': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜‹': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ¥³': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜‡': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',

            # Love and affection
            'ğŸ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ¥°': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ˜—': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜™': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ˜š': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'â¤ï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ§¡': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ’›': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ’š': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ’™': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ’œ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ¤': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ–¤': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ¤': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',

            # Positive gestures
            'ğŸ‘': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ™Œ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ‘': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ¤': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ¤—': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ¤²': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ‘‹': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'âœŒï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ¤': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',

            # Celebration
            'ğŸ‰': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'âœ¨': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸŒŸ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸŠ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸˆ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸ†': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ‡': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸ‚': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',

            # Nature and beauty
            'ğŸŒ¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸŒº': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸŒ¹': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'ğŸŒ»': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸŒ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸŒˆ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ',
            'â­': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 'ğŸŒ ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ'
        }

        self.negative_emojis = {
            # Sad faces
            'ğŸ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜”': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜Ÿ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ˜•': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜£': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜–': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ˜«': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜©': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜¢': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ˜­': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜¤': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜ ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ˜¡': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ¤¬': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜¨': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ˜°': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜¥': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜“': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ˜ª': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜´': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜µ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ˜µâ€ğŸ’«': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜·': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ¤’': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ¤•': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ¤¢': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ¤®': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ¤§': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',

            # Negative gestures
            'ğŸ‘': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ’”': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜¾': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ™€': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ˜¿': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ’¢': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ’¥': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ’«': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ’¦': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ’¨': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ•³ï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ’£': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ’©': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ¤¡': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ‘¹': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ‘º': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ‘»': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ‘½': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸ‘¾': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸ¤–': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',

            # Weather and nature
            'ğŸŒ§ï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'â›ˆï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸŒ©ï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸŒ¨ï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸŒªï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸŒ«ï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸŒš': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸŒ’': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸŒ“': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸŒ”': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸŒ•': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸŒ–': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ',
            'ğŸŒ—': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ', 'ğŸŒ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ù„Ø¨ÙŠ'
        }

        self.sarcastic_emojis = {
            # Sarcastic faces
            'ğŸ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜’': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ™„': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜‘': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤”': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ¤¨': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜¶': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜Œ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜ª': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜´': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜·': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ¤': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤«': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤­': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ¤¥': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜¬': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜®â€ğŸ’¨': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜¤': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜®': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜¯': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜²': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜³': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¥º': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜¦': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜§': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜¨': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜°': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜¥': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜“': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ¤—': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤”': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤«': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ¤': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤¨': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤¯': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜¶â€ğŸŒ«ï¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜¶': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜‘': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜’': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ™„': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜¬': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤¥': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜Œ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜”': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜ª': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜´': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜·': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤’': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ¤•': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤¢': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤®': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ¤§': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¥µ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¥¶': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ¥´': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜µ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ˜µâ€ğŸ’«': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ¤¯': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤ ': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¥¸': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±',
            'ğŸ˜': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ¤“': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±', 'ğŸ§': 'Ø±Ù…Ø² ØªØ¹Ø¨ÙŠØ±ÙŠ Ø³Ø§Ø®Ø±'
        }
        self.emoji_mappings = {**self.positive_emojis, **self.negative_emojis, **self.sarcastic_emojis}
        logger.info(f"Initialized EmojiHandler with {len(self.emoji_mappings)} emoji mappings")

    def process_emojis(self, text):
        """Convert emojis to meaningful text representations"""
        # First, get all emojis in the text
        emojis = emoji.emoji_list(text)

        if emojis:
            logger.debug(f"Found {len(emojis)} emojis in text: {text}")

        # Sort emojis by their position in reverse order to avoid position shifting
        emojis.sort(key=lambda x: x['match_start'], reverse=True)

        # Replace each emoji with its sentiment representation
        for emoji_info in emojis:
            emoji_char = emoji_info['emoji']
            if emoji_char in self.emoji_mappings:
                replacement = f" {self.emoji_mappings[emoji_char]} "
                text = text[:emoji_info['match_start']] + replacement + text[emoji_info['match_end']:]
                logger.debug(f"Replaced emoji {emoji_char} with {self.emoji_mappings[emoji_char]}")

        return text

# -*- coding: utf-8 -*-
"""
Generate a file of Arabic sentiment words (positive and negative)
to subtract from your stop-word list.
"""

# 1. Define a list of common positive sentiment words
#    (sourced from Kalimah Centerâ€™s â€œbeautiful Arabic wordsâ€) :contentReference[oaicite:0]{index=0}
positive_words = [
    "Ø¬Ù…ÙŠÙ„",   # beautiful
    "Ø±Ø§Ø¦Ø¹",   # wonderful
    "Ù…Ù…ØªØ§Ø²",  # excellent
    "Ù„Ø°ÙŠØ°",   # delicious
    "Ù…Ø±Ø­",    # cheerful
    "Ø³Ø¹ÙŠØ¯",   # happy
    "Ù…Ø¨ØªÙ‡Ø¬",  # joyful
    "Ù…ØªÙØ§Ø¦Ù„", # optimistic
    "Ù…ØªØ­Ù…Ø³",  # excited
    "Ù…Ø¨Ù‡Ø±",   # amazing
    "Ù…Ø°Ù‡Ù„",   # astonishing
    "Ù…Ø¨Ø´Ø±",   # promising
    "Ù…ÙØ¹Ù…",   # full of
    "Ù…Ø´Ø±Ù‚",   # bright
    "Ù…Ù†ÙŠØ±",   # illuminating
    "Ù…Ø¶ÙŠØ¡",   # shining
    "Ù…Ø¨Ù‡Ø±Ø¬",  # dazzling
]
# 2. Add more positive adjectives from Ling-Appâ€™s adjective list :contentReference[oaicite:1]{index=1}
positive_words += [
    "Ø°ÙƒÙŠ",    # intelligent
    "Ù„Ø·ÙŠÙ",   # kind
    "ÙƒØ±ÙŠÙ…",   # generous
    "Ù…ÙˆÙ‡ÙˆØ¨",  # talented
    "ÙˆØ¯ÙˆØ¯",   # friendly
    "Ù…Ø³Ø¤ÙˆÙ„",  # responsible
    "Ù…ÙÙŠØ¯",   # helpful
    "Ù…Ù„Ù‡Ù…",   # inspirational
    "Ù…ØªØ­Ù…Ø³",  # enthusiastic
]

# 3. Define a list of common negative sentiment words
#    (basic negative adjectives; you may expand this from domain knowledge)
negative_words = [
    "Ø³ÙŠØ¡",     # bad
    "ÙØ¸ÙŠØ¹",    # horrible
    "Ù‚Ø¨ÙŠØ­",    # ugly
    "Ø¨ØºÙŠØ¶",    # hateful
    "Ø­Ø²ÙŠÙ†",    # sad
    "Ø´Ø±ÙŠØ±",    # evil
    "ØºØ§Ø¶Ø¨",    # angry
    "Ø®Ø§Ø¦Ù",    # fearful
    "Ù…ØªÙˆØªØ±",   # nervous
    "ÙƒØ³ÙˆÙ„",    # lazy
    "Ù‚Ø°Ø±",     # dirty
    # Ø¥Ø¶Ø§ÙØ§Øª
    "Ù…Ù‚Ø±Ù",     # disgusting
    "Ø¨Ø´Ø¹",      # hideous
    "Ù…ØªØ¹Ø¬Ø±Ù",   # arrogant
    "Ù…ØªØ¹ØµØ¨",    # fanatic
    "Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ø´Ù…Ø¦Ø²Ø§Ø²", # revolting
    "ÙƒØ§Ø±Ø«ÙŠ",    # catastrophic
    "Ù…Ø¤Ù„Ù…",     # painful
    "Ù…Ø­Ø¨Ø·",     # frustrating
    "Ù…Ø¸Ù„Ù…",     # dark (neg. connotation)
    "Ù‚Ø§ØªÙ…",     # gloomy
    "Ù…Ø®ÙŠÙ",     # scary
    "Ù…Ø¤Ø°ÙŠ",     # harmful
    "Ù…ØªØ®Ù„Ù",    # backward (derogatory)
    "Ø¨Ø§Ø¦Ø³",     # miserable
    "Ù…Ø±ÙŠØ¶",     # sick (derog.)
    "Ø­Ù‚ÙŠØ±",     # despicable
    "ØºØ¨ÙŠ",      # stupid
    "Ø£Ø­Ù…Ù‚",     # fool
    "Ù…ØªØ¹ÙÙ†",    # rotten
    "Ù…Ø²Ø¹Ø¬",     # annoying
    "Ù…Ø«ÙŠØ± Ù„Ù„Ù‚Ù„Ù‚", # worrisome
    "Ù…ØºØ±Ø¶",     # malicious
    "ÙˆÙ‚Ø­",      # rude
    "Ø¹Ø¯ÙˆØ§Ù†ÙŠ",   # aggressive
    "ØºÙŠØ± Ù…ÙˆØ«ÙˆÙ‚", # untrustworthy
    "Ù…Ø³ØªÙØ²",    # provocative
    "Ù…ØªØ·Ø±Ù",    # extremist
    "Ø®Ø§Ø¦Ø¨ Ø§Ù„Ø£Ù…Ù„" # disappointed
]

# 4. Combine positive and negative into one sentiment list
sentiment_words = set(positive_words + negative_words)

# 5. Write them to file, one per line
with open("arabic_sentiment_words.txt", "w", encoding="utf-8") as outfile:
    for w in sorted(sentiment_words):
        outfile.write(w + "\n")

print(f"Generated 'arabic_sentiment_words.txt' with {len(sentiment_words)} entries.")

msa_stopwords = [
    "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù„Ù‰", "Ø¹Ù†", "ÙƒÙ…Ø§", "Ø¥Ù†", "Ø£Ù†",
    "Ù‡Ù„", "Ø«Ù…", "Ø­ØªÙ‰", "Ø¥Ø°Ø§", "Ø¥Ø°", "Ø¨ÙŠÙ†", "ÙƒÙ„", "Ø¹Ù„Ù‰", "Ù…Ø¹", "Ø¨Ø¹Ø¯", "Ù‚Ø¨Ù„",
    "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "Ù‡Ù†Ø§Ùƒ", "Ù‡Ù†Ø§", "Ø£ÙŠ", "Ø£Ùˆ", "Ø£Ù…", "Ùˆ", "Ù"
]
egyptian_stopwords = [
    "ÙŠØ¹Ù†ÙŠ", "Ø·Ø¨", "Ø¨Ø³", "ØªÙ…Ø§Ù…",  "Ø¨Øµ", "Ø®Ù„Ø§Øµ",
    "Ù…Ø´", "Ø£ÙˆÙŠ", "Ø¨Ø¹Ø¯ÙŠÙ†", "Ù…Ø§Ø´ÙŠ", "ÙˆÙ„Ø§",  "Ø§Ø¯ÙŠ", "Ø¯Ù„ÙˆÙ‚ØªÙŠ", "Ø¯Ù„ÙˆÙ‚Øª", "Ø­Ø§Ø¬Ø©", "Ø¯Ø§"
]
levantine_stopwords = [
    "ÙŠØ¹Ù†ÙŠ", "Ø£ÙŠÙ‡", "ÙƒØªÙŠØ±", "Ù…Ø´Ø§Ù†", "ÙˆÙŠÙ†", "Ø®ÙŠÙŠ", "Ø´Ùˆ", "Ù„Ø³Ø§",
    "ÙŠØ¹Ù†ÙŠ", "ÙŠÙ„Ø§", "Ø®Ù„Øµ", "Ù…Ø§Ø´ÙŠ", "Ø£ÙˆÙƒÙŠ", "Ù‚Ø§Ø¹Ø¯", "ÙƒØ§Ù†", "Ø¨Ø¯Ù‘ÙŠ"
]
gulf_stopwords = [
    "ÙŠØ¹Ù†ÙŠ", "Ø·Ø¨Ø¹Ù‹Ø§", "Ù…Ø±Ø©", "ÙŠÙˆÙ…", "Ø¹Ù…", "Ø´Ù„ÙˆÙ†", "Ø­ÙŠØ§Ùƒ", "Ø¯Ø¨Ø±",
    "ÙŠØµÙŠØ±", "Ø¹Ø§Ø¯", "Ù‡ÙˆØ§ÙŠ", "Ù‡Ø°Ø§Ùƒ", "ØªÙ…Ø§Ù…", "Ø­Ø§Ø¶Ø±", "Ù„Ø§Ø²Ù…", "ÙŠØ¹Ù†Ù‘ÙŠ",
    "Ø±Ù…Ø²","ØªØ¹Ø¨ÙŠØ±ÙŠ","ÙŠØ§","Ø¯Ù‡","Ø§Ù„Ø¹Ø§Ù„Ù…","Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠÙ‡","Ø§ÙˆØ¨Ø§Ù…Ø§","Ø³ÙˆØ±ÙŠØ§",
    "Ø§Ù„Ø±ÙŠÙŠØ³","Ø­ØªÙŠ","Ø­Ù„Ø¨","Ù…ØµØ±","Ø§Ù„Ù„ÙŠ","Ø±ÙˆØ³ÙŠØ§","Ø¯ÙŠ","Ù‡Ø§Ø±ÙŠ","Ø¨ÙˆØªØ±","Ø§Ù„",
    "Ø§Ø­Ù…Ø¯","ØªØ±Ø§Ù…Ø¨","Ù…Ø¯Ø±ÙŠØ¯","Ù…ÙŠØ³ÙŠ","Ø¹Ø´Ø§Ù†","ÙˆÙŠÙ†Ø¯ÙˆØ²","Ø§ÙŠØ±Ø§Ù†","Ø§Ù„Ø¹Ø±Ø¨ÙŠÙ‡",
    "Ø´ÙŠ","Ø§ÙŠÙ‡","ÙƒÙ„Ù†ØªÙˆÙ†","Ø±Ù…Ø¶Ø§Ù†","Ø¨Ø´Ø§Ø±","Ø§Ù„Ø§Ø³Ø¯","Ø§Ù„Ø´ÙŠØ®","ØºÙˆØºÙ„","Ù…Ø­Ù…Ø¯",
    "Ø­Ø§Ø¬Ù‡","Ø§Ø®Ø±","ÙƒÙ†Øª","Ø¹Ø¨Ø±","Ø¨Ùˆ","Ù„Ø¨Ù†Ø§Ù†","Ø±ÙŠÙŠØ³","Ø§Ø¨Ùˆ","ÙÙŠØ¯Ø±Ø±","Ø¯ÙˆÙ†Ø§Ù„Ø¯"
    ,"Ø§Ø¨Ù„","Ø³ÙŠØ³ÙŠ","Ø§Ù„Ø¹Ø±Ø§Ù‚","ÙŠÙ‚ÙˆÙ„","Ø³Ù†Ù‡","ØºÙˆØªØ´ÙŠ","Ø§Ù„Ø¯ÙˆÙ„ÙŠ","Ø¬ÙˆØ¬Ù„","Ø¬Ø³ØªÙ†",
    "Ø§Ù†ÙŠ","Ø§Ù…Ø±ÙŠÙƒØ§","Ø¨Ø±Ø´Ù„ÙˆÙ†Ù‡","Ø¨ÙŠØ¨Ø±","Ø§Ù†Ùƒ","Ø§Ø±Ø¯ÙˆØºØ§Ù†","Ø­Ø¯","Ø§Ù„Ù…ØµØ±ÙŠ","Ø§Ù„Ø´Ø±Ù‚ÙŠÙ‡"
    ,"Ø´ÙŠØ¡","Ø²ÙŠ","Ø§Ø±Ø¯ÙˆØºØ§Ù†","ÙÙŠØ¯ÙŠÙˆ","Ø§Ù…Ø§Ø²ÙˆÙ†","ÙˆØ²ÙŠØ±","Ù…Ø¯ÙŠÙ†Ù‡","Ø³ÙˆØ±ÙŠÙ‡","Ø§Ù„Ø³ÙŠØ³ÙŠ",
    "Ø§Ù„Ø³ÙˆØ±ÙŠÙŠÙ†","Ø¨ÙˆÙƒÙŠÙ…ÙˆÙ†","Ø¨Ø±ÙŠØ·Ø§Ù†ÙŠØ§","Ø¨ÙŠØª","Ù…Ø¬Ù„Ø³","Ø§Ù„Ù†Ø§ØªÙˆ","Ø¯ÙˆÙ„Ù‡","Ø§Ù„ÙŠÙ…ÙŠÙ†",
    "Ø¯ÙˆÙ„","Ø§Ù„ØºØ±Ø¨ÙŠ","Ø§Ù„ÙˆØ³Ø·","Ù„Ø§Ù†","Ø¨Ù‚ÙŠ","Ø¨ÙˆØªÙ†","Ø§Ù„Ø¬Ø§Ù…Ø¹Ù‡","Ø¹Ù„ÙŠÙ‡Ù…","Ø¹Ù„ÙŠÙƒÙ…"
]

nltk.download("stopwords")
from nltk.corpus import stopwords
with open("/kaggle/input/stop-words/list (1).txt", encoding="utf8") as f:
    arabic_sw = {w.strip() for w in f if w.strip()}

# 3. Load an Arabic sentiment lexicon (so these words wonâ€™t be removed)
#    Here we assume a tab-separated file â€œArSenL.txtâ€ with <word>\t<polarity> per line.
sentiment_words = set()
with open("/kaggle/working/arabic_sentiment_words.txt", encoding="utf8") as lex:
    for line in lex:
        parts = line.strip().split()
        if parts:
            sentiment_words.add(parts[0])

sarcasm_cues = {
    "Ù‡Ù‡Ù‡Ù‡", "Ø¹ÙŠØ¨", "ÙŠØ§ Ø³Ù„Ø§Ù…", "Ø­Ù‚Ù‹Ø§", "Ø·Ø¨Ø¹Ù‹Ø§", "Ø·Ø¨Ø¹Ø§Ù‹", "Ø£ÙŠÙˆÙ‡", "Ø£Ù‡Ø§"
}

# 5. Build the filtered stop-word set
filtered_sw = arabic_sw - sentiment_words - sarcasm_cues

print("Number of filtered stopwords:", len(filtered_sw))

class ArabicTextPreprocessor:
    def __init__(self):
        # Initialize Arabic stopwords
        with open("stop_list.txt", encoding="utf8") as f:
            filtered_sw = {w.strip() for w in f if w.strip()}

        self.stop_words = set(filtered_sw)
        # Define Arabic and English punctuation
        self.punctuations = string.punctuation + 'ØŒØ›ØŸ'
        # Define Arabic diacritics pattern
        self.arabic_diacritics = re.compile("""
                                         Ù‘    | # Tashdid
                                         Ù    | # Fatha
                                         Ù‹    | # Tanwin Fath
                                         Ù    | # Damma
                                         ÙŒ    | # Tanwin Damm
                                         Ù    | # Kasra
                                         Ù    | # Tanwin Kasr
                                         Ù’    | # Sukun
                                         Ù€     # Tatwil/Kashida
                                         """, re.VERBOSE)

        # Define common Arabic noise patterns
        self.noise_patterns = {
            r'[^\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF\s]': '',  # Keep only Arabic characters
            r'\s+': ' ',  # Replace multiple spaces with single space
            r'[^\w\s]': '',  # Remove special characters
            r'[a-zA-Z]': '',  # Remove English characters
        }

        # Initialize emoji handler
        self.emoji_handler = EmojiHandler()
        logger.info("Initialized ArabicTextPreprocessor")



    def remove_duplicate_phrases(self, text, max_phrase_length=3):
        """
        Remove duplicate phrases of length 2 or more words while preserving word order.
        Args:
            text: Input text
            max_phrase_length: Maximum length of phrases to check for duplicates (default: 3)
        """
        if not text:
            return text

        words = text.split()
        if len(words) < 2:
            return text

        # Process phrases of different lengths
        for phrase_length in range(max_phrase_length, 1, -1):
            i = 0
            while i < len(words) - phrase_length:
                # Get the current phrase
                current_phrase = ' '.join(words[i:i + phrase_length])

                # Look for this phrase in the rest of the text
                j = i + phrase_length
                while j < len(words) - phrase_length + 1:
                    next_phrase = ' '.join(words[j:j + phrase_length])

                    if current_phrase == next_phrase:
                        # Remove the duplicate phrase
                        words = words[:j] + words[j + phrase_length:]
                    else:
                        j += 1
                i += 1

        return ' '.join(words)

    def remove_duplicate_words(self, text):
        """
        Remove duplicate words while preserving word order.
        Only removes consecutive duplicates to maintain context.
        """
        if not text:
            return text

        words = text.split()
        if not words:
            return text

        # Only remove consecutive duplicates
        result = []
        prev_word = None

        for word in words:
            # Only add the word if it's different from the previous word
            if word != prev_word:
                result.append(word)
                prev_word = word

        return ' '.join(result)

    def remove_diacritics(self, text):
        """Remove Arabic diacritics from text"""
        text = re.sub(self.arabic_diacritics, '', text)
        return text

    def remove_urls(self, text):
        """Remove URLs from text"""
        url_pattern = re.compile(r'https?://\S+|www\.\S+')
        return url_pattern.sub('', text)

    def remove_mentions(self, text):
        """Remove Twitter mentions"""
        return re.sub(r'@\w+', '', text)

    def remove_hashtags(self, text):
        """Remove hashtags"""
        return re.sub(r'#\w+', '', text)

    def remove_numbers(self, text):
        """Remove numbers"""
        return re.sub(r'\d+', '', text)

    def remove_punctuations(self, text):
        """Remove punctuations"""
        translator = str.maketrans('', '', self.punctuations)
        return text.translate(translator)

    def remove_stopwords(self, text):
        """Remove Arabic stopwords"""
        words = text.split()
        return ' '.join([word for word in words if word not in self.stop_words])

    def normalize_arabic_text(self, text):
        """Normalize Arabic text by standardizing characters"""
        # Normalize Alef variations
        text = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
        # Normalize Ya variations
        text = re.sub("Ù‰", "ÙŠ", text)
        # Normalize Ta Marbuta
        text = re.sub("Ø©", "Ù‡", text)
        # Normalize Kaf
        text = re.sub("Ú¯", "Ùƒ", text)
        # Normalize Waw
        text = re.sub("Ø¤", "Ùˆ", text)
        # Normalize Ya with Hamza
        text = re.sub("Ø¦", "ÙŠ", text)
        return text

    def remove_noise(self, text):
        """Remove noise patterns from text"""
        for pattern, replacement in self.noise_patterns.items():
            text = re.sub(pattern, replacement, text)
        return text

    def preprocess(self, text):
        """Apply all preprocessing steps to the text"""
        if not isinstance(text, str):
            return ""

        # Process emojis first
        original_text = text
        text = self.emoji_handler.process_emojis(text)

        if text != original_text:
            logger.info(f"Emoji processing changed text from '{original_text}' to '{text}'")




        # Apply other preprocessing steps
        text = self.remove_urls(text)
        text = self.remove_mentions(text)
        text = self.remove_hashtags(text)
        text = self.remove_numbers(text)
        text = self.remove_punctuations(text)
        text = self.normalize_arabic_text(text)
        text = self.remove_diacritics(text)
        text = self.remove_noise(text)
        text = self.remove_stopwords(text)
        text = self.remove_duplicate_words(text)
        # Remove duplicate phrases
        text = self.remove_duplicate_phrases(text)
        # Remove extra whitespace
        text = ' '.join(text.split())
        return text

class MLDataPreparator:
    def __init__(self):
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=51000,  # Increased from 5000
            ngram_range=(1, 3),  # Increased from (1, 2)
            min_df=3,
            max_df=0.95,
            sublinear_tf=True
        )
        self.sentiment_encoder = LabelEncoder()
        self.sarcasm_encoder = LabelEncoder()
        self.scaler = StandardScaler()

    def prepare_data(self, df):
        """Prepare data for ML algorithms with enhanced features"""
        # Select relevant features
        features = ['processed_text', 'sentiment', 'sarcasm']
        df = df[features].copy()

        # Remove rows with empty processed text
        df = df[df['processed_text'].str.strip() != '']

        # Remove very short texts (less than 3 words)
        df = df[df['processed_text'].str.split().str.len() >= 3]

        # Remove very long texts (more than 100 words)
        df = df[df['processed_text'].str.split().str.len() <= 50]

        # Add text length as a feature
        df['text_length'] = df['processed_text'].str.len()

        # Add word count as a feature
        df['word_count'] = df['processed_text'].str.split().str.len()

        # Add average word length as a feature
        df['avg_word_length'] = df['processed_text'].str.split().apply(
            lambda x: np.mean([len(word) for word in x]) if x else 0
        )

        # Create TF-IDF features
        X_tfidf = self.tfidf_vectorizer.fit_transform(df['processed_text'])
        print(f"TF-IDF matrix shape: {X_tfidf.shape}")

        print("\nCreating additional features...")
        # Create additional features
        X_additional = df[['text_length', 'word_count', 'avg_word_length']].values
        X_additional = self.scaler.fit_transform(X_additional)

        # Combine features
        X = sparse.hstack([X_tfidf, X_additional])
        print(f"Final feature matrix shape: {X.shape}")

        # Encode labels
        print("\nEncoding labels...")
        df['sentiment_encoded'] = self.sentiment_encoder.fit_transform(df['sentiment'])
        df['sarcasm_encoded'] = self.sarcasm_encoder.fit_transform(df['sarcasm'])

        # Create target variables
        y_sentiment = df['sentiment_encoded']
        y_sarcasm = df['sarcasm_encoded']

        # Split the data with stratification
        print("\nSplitting data into train and test sets...")
        X_train, X_test, y_sentiment_train, y_sentiment_test = train_test_split(
            X, y_sentiment, test_size=0.2, random_state=42, stratify=y_sentiment
        )

        _, _, y_sarcasm_train, y_sarcasm_test = train_test_split(
            X, y_sarcasm, test_size=0.2, random_state=42, stratify=y_sarcasm
        )

        print(f"Training set shape: {X_train.shape}")
        print(f"Test set shape: {X_test.shape}")

        return {
            'X_train': X_train,
            'X_test': X_test,
            'y_sentiment_train': y_sentiment_train,
            'y_sentiment_test': y_sentiment_test,
            'y_sarcasm_train': y_sarcasm_train,
            'y_sarcasm_test': y_sarcasm_test,
            'feature_names': self.tfidf_vectorizer.get_feature_names_out(),
            'sentiment_classes': self.sentiment_encoder.classes_,
            'sarcasm_classes': self.sarcasm_encoder.classes_
        }

# Load the dataset
print("Loading dataset...")
df = pd.read_csv('/kaggle/input/arabic-dataset/ArSarcasm.csv')

# Display initial information
print("\nInitial dataset shape:", df.shape)
print("\nInitial columns:", df.columns.tolist())

print(df.head())

print(df.tail(10))

# define mapping
sentiment_map = {
    'NEG': 'negative',
    'POS': 'positive',
    'NEU': 'neutral'
}

# apply mapping inâ€place
df['sentiment'] = df['sentiment'].replace(sentiment_map)

print(df.tail(10))

df = df.sample(frac=1, random_state=42).reset_index(drop=True)

df.head()

# Initialize preprocessor
preprocessor = ArabicTextPreprocessor()

# Preprocess the text
print("\nPreprocessing text...")
df['processed_text'] = df['tweet'].apply(preprocessor.preprocess)

# Initialize ML data preparator
ml_preparator = MLDataPreparator()

# Prepare data for ML
print("\nPreparing data for ML...")
ml_data = ml_preparator.prepare_data(df)

# Display data preparation results
print("\nData Preparation Results:")
print(f"Training set size: {ml_data['X_train'].shape[0]} samples")
print(f"Test set size: {ml_data['X_test'].shape[0]} samples")
print(f"Number of features: {ml_data['X_train'].shape[1]}")
print("\nSentiment classes:", ml_data['sentiment_classes'])
print("Sarcasm classes:", ml_data['sarcasm_classes'])

# Display some examples of processed text
logger.info("\nExamples of processed text:")
print("\nExamples of processed text:")
for i in range(5):
    print(f"\nOriginal: {df['tweet'].iloc[i]}")
    print(f"Processed: {df['processed_text'].iloc[i]}")
    print(f"Sentiment: {df['sentiment'].iloc[i]}")
    print(f"Sarcasm: {df['sarcasm'].iloc[i]}")

import os
import scipy.sparse as sparse

# ... (rest of your existing code) ...

# Create the 'dataset' directory if it doesn't exist
os.makedirs('dataset', exist_ok=True)

# Save prepared data
sparse.save_npz('dataset/X_train.npz', ml_data['X_train'])
sparse.save_npz('dataset/X_test.npz', ml_data['X_test'])
np.save('dataset/y_sentiment_train.npy', ml_data['y_sentiment_train'])
np.save('dataset/y_sentiment_test.npy', ml_data['y_sentiment_test'])
np.save('dataset/y_sarcasm_train.npy', ml_data['y_sarcasm_train'])
np.save('dataset/y_sarcasm_test.npy', ml_data['y_sarcasm_test'])

# Save feature names and class mappings
np.save('dataset/feature_names.npy', ml_data['feature_names'])
np.save('dataset/sentiment_classes.npy', ml_data['sentiment_classes'])
np.save('dataset/sarcasm_classes.npy', ml_data['sarcasm_classes'])

logger.info("Prepared data saved to dataset/ directory")

class ModelTrainer:
    def __init__(self):
        self.models = {

            'rf': RandomForestClassifier(
                n_estimators=200,
                max_depth=20,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
        }
        self.scaler = StandardScaler()
        self.best_model = None
        self.best_score = 0
        self.label_encoders = {
            'sentiment': LabelEncoder(),
            'sarcasm': LabelEncoder()
        }

    def load_data(self):
        """Load preprocessed data"""
        logger.info("Loading preprocessed data...")

        # Load sparse matrices
        X_train = sparse.load_npz('dataset/X_train.npz')
        X_test = sparse.load_npz('dataset/X_test.npz')

        # Load labels
        y_sentiment_train = np.load('dataset/y_sentiment_train.npy')
        y_sentiment_test = np.load('dataset/y_sentiment_test.npy')
        y_sarcasm_train = np.load('dataset/y_sarcasm_train.npy')
        y_sarcasm_test = np.load('dataset/y_sarcasm_test.npy')

        # Load feature names and class mappings
        feature_names = np.load('dataset/feature_names.npy', allow_pickle=True)
        sentiment_classes = np.load('dataset/sentiment_classes.npy', allow_pickle=True)
        sarcasm_classes = np.load('dataset/sarcasm_classes.npy', allow_pickle=True)

        # Ensure labels are properly encoded
        y_sentiment_train = self.label_encoders['sentiment'].fit_transform(y_sentiment_train)
        y_sentiment_test = self.label_encoders['sentiment'].transform(y_sentiment_test)
        y_sarcasm_train = self.label_encoders['sarcasm'].fit_transform(y_sarcasm_train)
        y_sarcasm_test = self.label_encoders['sarcasm'].transform(y_sarcasm_test)

        return {
            'X_train': X_train,
            'X_test': X_test,
            'y_sentiment_train': y_sentiment_train,
            'y_sentiment_test': y_sentiment_test,
            'y_sarcasm_train': y_sarcasm_train,
            'y_sarcasm_test': y_sarcasm_test,
            'feature_names': feature_names,
            'sentiment_classes': sentiment_classes,
            'sarcasm_classes': sarcasm_classes
        }

    def train_traditional_models(self, X_train, y_train, task='sentiment'):
        """Train traditional ML models with enhanced parameters"""
        logger.info(f"Training traditional models for {task} classification...")

        # Check class balance and apply SMOTE if needed
        class_counts = np.bincount(y_train)
        logger.info(f"Class counts for {task}: {class_counts}")

        if len(class_counts) > 1 and class_counts.min() < 100:
            logger.info(f"Applying SMOTE for {task} data...")
            sm = SMOTE(random_state=42, k_neighbors=5)
            X_train, y_train = sm.fit_resample(X_train, y_train)

        results = {}
        for name, model in self.models.items():
            logger.info(f"Training {name}...")

            # Train model with cross-validation
            cv_scores = cross_val_score(
                model, X_train, y_train,
                cv=5,
                scoring='f1_weighted',
                n_jobs=-1
            )
            mean_cv_score = cv_scores.mean()

            # Train final model on full training set
            model.fit(X_train, y_train)

            results[name] = {
                'model': model,
                'cv_score': mean_cv_score
            }

            logger.info(f"{name} CV Score: {mean_cv_score:.4f}")

            # Update best model
            if mean_cv_score > self.best_score:
                self.best_score = mean_cv_score
                self.best_model = model

        return results

    def train_deep_learning_model(self, X_train, y_train, task='sentiment'):
        """Train enhanced deep learning model"""
        logger.info(f"Training deep learning model for {task} classification...")

        # Convert sparse matrix to dense
        X_train_dense = X_train.toarray()

        # Reshape input for LSTM
        X_train_reshaped = X_train_dense.reshape(X_train_dense.shape[0], 1, X_train_dense.shape[1])

        # Build enhanced LSTM model
        model = Sequential([
            LSTM(256, input_shape=(1, X_train_dense.shape[1]), return_sequences=True),
            Dropout(0.3),
            LSTM(128, return_sequences=True),
            Dropout(0.3),
            LSTM(64),
            Dropout(0.3),
            Dense(128, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            Dense(len(np.unique(y_train)), activation='softmax')
        ])

        # Compile model with better optimizer settings
        optimizer = tf.keras.optimizers.Adam(
            learning_rate=0.001,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-07
        )

        model.compile(
            optimizer=optimizer,
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        # Add callbacks
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.2,
                patience=3,
                min_lr=0.00001
            )
        ]

        # Train model with better batch size and validation split
        history = model.fit(
            X_train_reshaped, y_train,
            epochs=50,
            batch_size=64,
            validation_split=0.2,
            callbacks=callbacks,
            verbose=1
        )

        return model, history

    def evaluate_single_model(self, model, X_test, y_test):
        """Enhanced evaluation of a single model"""
        y_test = np.array(y_test)

        try:
            if isinstance(model, tf.keras.Model):
                X_test_dense = X_test.toarray()
                X_test_reshaped = X_test_dense.reshape(X_test_dense.shape[0], 1, X_test_dense.shape[1])
                y_pred_proba = model.predict(X_test_reshaped)
                y_pred = np.argmax(y_pred_proba, axis=1)
            elif hasattr(model, 'predict_proba'):
                y_pred_proba = model.predict_proba(X_test)
                y_pred = np.argmax(y_pred_proba, axis=1)
            else:
                y_pred = model.predict(X_test)

            y_pred = np.array(y_pred)
            y_test_1d = y_test.ravel()
            y_pred_1d = y_pred.ravel()
            y_pred_1d = y_pred_1d.astype(y_test_1d.dtype)

            # Calculate detailed metrics
            report = classification_report(y_test_1d, y_pred_1d, output_dict=True)
            conf_matrix = confusion_matrix(y_test_1d, y_pred_1d)

            # Calculate additional metrics
            precision = precision_score(y_test_1d, y_pred_1d, average='weighted')
            recall = recall_score(y_test_1d, y_pred_1d, average='weighted')
            f1 = f1_score(y_test_1d, y_pred_1d, average='weighted')

            return {
                'classification_report': report,
                'confusion_matrix': conf_matrix,
                'predictions': y_pred_1d,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            }

        except Exception as e:
            logger.error(f"Error evaluating model: {str(e)}")
            return None

    def evaluate_models(self, models, X_test, y_test, task='sentiment'):
        """Evaluate all models on test set"""
        logger.info(f"Evaluating models for {task} classification...")

        # Convert y_test to numpy array if it isn't already
        y_test = np.array(y_test)

        results = {}
        for name, model_info in models.items():
            model = model_info['model']

            try:
                # Get predictions
                if hasattr(model, 'predict_proba'):
                    y_pred_proba = model.predict_proba(X_test)
                    y_pred = np.argmax(y_pred_proba, axis=1)
                else:
                    y_pred = model.predict(X_test)

                # Convert predictions to numpy array
                y_pred = np.array(y_pred)

                # Ensure both arrays are 1D
                y_test_1d = y_test.ravel()
                y_pred_1d = y_pred.ravel()

                # Ensure both arrays have the same dtype
                y_pred_1d = y_pred_1d.astype(y_test_1d.dtype)

                # Calculate metrics
                report = classification_report(y_test_1d, y_pred_1d, output_dict=True)
                conf_matrix = confusion_matrix(y_test_1d, y_pred_1d)

                results[name] = {
                    'classification_report': report,
                    'confusion_matrix': conf_matrix,
                    'predictions': y_pred_1d
                }

                logger.info(f"\nResults for {name}:")
                logger.info(f"Accuracy: {report['accuracy']:.4f}")
                logger.info(f"Precision: {report['weighted avg']['precision']:.4f}")
                logger.info(f"Recall: {report['weighted avg']['recall']:.4f}")
                logger.info(f"F1-score: {report['weighted avg']['f1-score']:.4f}")

            except Exception as e:
                logger.error(f"Error evaluating model {name}: {str(e)}")
                logger.error(f"y_test shape: {y_test.shape}, dtype: {y_test.dtype}")
                logger.error(f"y_pred shape: {y_pred.shape}, dtype: {y_pred.dtype}")
                raise

        return results

    def plot_results(self, results, task='sentiment'):
        """Plot evaluation results"""
        # Create results directory if it doesn't exist
        os.makedirs('results', exist_ok=True)

        # Plot confusion matrices
        fig, axes = plt.subplots(2, 2, figsize=(15, 15))
        axes = axes.ravel()

        for idx, (name, result) in enumerate(results.items()):
            sns.heatmap(
                result['confusion_matrix'],
                annot=True,
                fmt='d',
                cmap='Blues',
                ax=axes[idx]
            )
            axes[idx].set_title(f'Confusion Matrix - {name}')

        plt.tight_layout()
        plt.savefig(f'results/{task}_confusion_matrices.png')
        plt.close()

        # Plot metrics comparison
        metrics = ['precision', 'recall', 'f1-score']
        model_names = list(results.keys())

        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        for idx, metric in enumerate(metrics):
            values = [
                results[name]['classification_report']['weighted avg'][metric]
                for name in model_names
            ]
            axes[idx].bar(model_names, values)
            axes[idx].set_title(f'{metric.capitalize()}')
            axes[idx].set_ylim(0, 1)

        plt.tight_layout()
        plt.savefig(f'results/{task}_metrics_comparison.png')
        plt.close()

    def save_models(self, models, task='sentiment'):
        """Save trained models"""
        logger.info(f"Saving models for {task} classification...")

        # Create models directory if it doesn't exist
        os.makedirs('models', exist_ok=True)

        # Check if 'models' is a dictionary or a Keras model
        if isinstance(models, dict):
            # If it's a dictionary, save each model using joblib
            for name, model_info in models.items():
                model = model_info['model']
                joblib.dump(model, f'models/{task}_{name}_model.joblib')
        else:
            # If it's a Keras model, save it using Keras's save method
            models.save(f'models/{task}_dl_model.keras')

        # Save label encoder
        joblib.dump(self.label_encoders[task], f'models/{task}_label_encoder.joblib')

def configure_gpu():
    """Configure GPU settings for optimal performance"""
    # Enable memory growth to prevent GPU memory from being fully allocated
    gpus = tf.config.list_physical_devices('T4 GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            logger.info(f"Found {len(gpus)} GPU(s). GPU memory growth enabled.")

            # Enable mixed precision for faster training
            policy = mixed_precision.Policy('mixed_float16')
            mixed_precision.set_global_policy(policy)
            logger.info("Mixed precision enabled for faster training.")

            # Set visible devices to GPU
            tf.config.set_visible_devices(gpus[0], 'GPU')
            logger.info(f"Using GPU: {gpus[0]}")

        except RuntimeError as e:
            logger.error(f"GPU configuration error: {e}")
    else:
        logger.warning("No GPU found. Running on CPU.")

configure_gpu()

trainer = ModelTrainer()
data=trainer.load_data()

# Train and evaluate sentiment models
sentiment_models = trainer.train_traditional_models(
    data['X_train'],
    data['y_sentiment_train'],
    task='sentiment'
)

sentiment_results = trainer.evaluate_models(
    sentiment_models,
    data['X_test'],
    data['y_sentiment_test'],
    task='sentiment'
)

print(sentiment_results)

# Train and evaluate sarcasm models
sarcasm_models = trainer.train_traditional_models(
    data['X_train'],
    data['y_sarcasm_train'],
    task='sarcasm'
)

sarcasm_results = trainer.evaluate_models(
    sarcasm_models,
    data['X_test'],
    data['y_sarcasm_test'],
    task='sarcasm'
)

# Plot results
trainer.plot_results(sentiment_results, task='sentiment')

trainer.plot_results(sarcasm_results, task='sarcasm')

# Save models
trainer.save_models(sentiment_models, task='sentiment')
trainer.save_models(sarcasm_models, task='sarcasm')
logger.info("Training and evaluation completed!")

# Train and evaluate sentiment models
sentiment_models, history = trainer.train_deep_learning_model(
    data['X_train'],
    data['y_sentiment_train'],
    task='sentiment'
)



sentiment_results = trainer.evaluate_single_model(
    sentiment_models,
    data['X_test'],
    data['y_sentiment_test']
)

# Train and evaluate sarcasm models
sarcasm_models,history = trainer.train_deep_learning_model(
    data['X_train'],
    data['y_sarcasm_train'],
    task='sarcasm'
)

sarcasm_results = trainer.evaluate_single_model(
    sarcasm_models,
    data['X_test'],
    data['y_sarcasm_test'],
)

# Save models
trainer.save_models(sentiment_models, task='sentiment')
trainer.save_models(sarcasm_models, task='sarcasm')
logger.info("Training and evaluation completed!")

print("\nDeep Learning Model (LSTM):")
print(f"Accuracy: {sentiment_results['classification_report']['accuracy']:.4f}")
print(f"Precision: {sentiment_results['classification_report']['weighted avg']['precision']:.4f}")
print(f"Recall: {sentiment_results['classification_report']['weighted avg']['recall']:.4f}")
print(f"F1-score: {sentiment_results['classification_report']['weighted avg']['f1-score']:.4f}")

print("\nDeep Learning Model (LSTM):")
print(f"Accuracy: {sarcasm_results['classification_report']['accuracy']:.4f}")
print(f"Precision: {sarcasm_results['classification_report']['weighted avg']['precision']:.4f}")
print(f"Recall: {sarcasm_results['classification_report']['weighted avg']['recall']:.4f}")
print(f"F1-score: {sarcasm_results['classification_report']['weighted avg']['f1-score']:.4f}")